{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd86cc3-a12f-48a7-b7fe-33338092535b",
   "metadata": {},
   "source": [
    "# no use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cd393-adef-4b80-9953-f994cc40483f",
   "metadata": {},
   "source": [
    "# new test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31b909-8495-4b37-8ff4-9fa7b9debcf2",
   "metadata": {},
   "source": [
    "**Working code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9f06e3a-bb32-4971-88d5-d2b11e690792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 44.9711\n",
      "Epoch 2, Loss: 32.7199\n",
      "Epoch 3, Loss: 29.2735\n",
      "Epoch 4, Loss: 27.8323\n",
      "Epoch 5, Loss: 26.8528\n",
      "Epoch 6, Loss: 25.8607\n",
      "Epoch 7, Loss: 25.2012\n",
      "Epoch 8, Loss: 24.5287\n",
      "Epoch 9, Loss: 23.5397\n",
      "Epoch 10, Loss: 23.2615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-AC       0.72      0.54      0.62       270\n",
      "        B-LF       0.27      0.11      0.16       150\n",
      "         B-O       0.90      0.95      0.93      4292\n",
      "        I-LF       0.39      0.27      0.32       288\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.57      0.47      0.51      5000\n",
      "weighted avg       0.84      0.87      0.85      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import classification_report\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Define the RNNModel class\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_dim, n_layers=1, dropout=0.3):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_vectors):\n",
    "        rnn_out, _ = self.rnn(input_vectors)\n",
    "        output = self.fc(rnn_out)\n",
    "        return output\n",
    "\n",
    "# Corrected function for preparing data\n",
    "def prepare_data(dataset, word2vec_model, label_encoding, max_len=128):\n",
    "    input_vectors = []\n",
    "    labels = []\n",
    "    \n",
    "    for data in dataset:\n",
    "        tokens = data['tokens']\n",
    "        ner_tags = data['ner_tags']\n",
    "        \n",
    "        # Convert tokens to word vectors\n",
    "        word_vectors = []\n",
    "        for token in tokens:\n",
    "            if token in word2vec_model:\n",
    "                word_vectors.append(word2vec_model[token])\n",
    "            else:\n",
    "                # Handle out-of-vocabulary words\n",
    "                word_vectors.append(np.zeros(word2vec_model.vector_size))\n",
    "        \n",
    "        # Pad or truncate word vectors to max_len\n",
    "        if len(word_vectors) > max_len:\n",
    "            word_vectors = word_vectors[:max_len]\n",
    "        else:\n",
    "            pad_length = max_len - len(word_vectors)\n",
    "            word_vectors.extend([np.zeros(word2vec_model.vector_size)] * pad_length)\n",
    "        \n",
    "        input_vectors.append(np.array(word_vectors))  # Convert to numpy array\n",
    "        \n",
    "        # Convert NER tags to numerical labels, padding or truncating to max_len\n",
    "        numerical_tags = [label_encoding.get(tag, -1) for tag in ner_tags]\n",
    "        if len(numerical_tags) > max_len:\n",
    "            numerical_tags = numerical_tags[:max_len]\n",
    "        else:\n",
    "            numerical_tags += [-1] * (max_len - len(numerical_tags))  \n",
    "        \n",
    "        labels.append(torch.tensor(numerical_tags))\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    input_vectors = torch.tensor(input_vectors, dtype=torch.float32) \n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return TensorDataset(input_vectors, labels) \n",
    "\n",
    "def train_model(model, train_loader, num_epochs=10, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for input_vectors, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_vectors)\n",
    "            logits_flat = outputs.view(-1, outputs.shape[-1])\n",
    "            labels_flat = labels.view(-1)\n",
    "            loss = criterion(logits_flat, labels_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "def evaluate_model(model, test_loader, label_encoding):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    with torch.no_grad():\n",
    "        for input_vectors, labels in test_loader:\n",
    "            outputs = model(input_vectors)\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            predictions.append(preds.cpu().numpy())\n",
    "            ground_truth.append(labels.cpu().numpy())\n",
    "    flat_predictions = np.concatenate([arr.flatten() for arr in predictions])\n",
    "    flat_ground_truth = np.concatenate([arr.flatten() for arr in ground_truth])\n",
    "    valid_indices = flat_ground_truth != -1\n",
    "    filtered_predictions = flat_predictions[valid_indices]\n",
    "    filtered_ground_truth = flat_ground_truth[valid_indices]\n",
    "    reverse_label_encoding = {v: k for k, v in label_encoding.items()}\n",
    "    filtered_predictions_labels = [reverse_label_encoding[pred] for pred in filtered_predictions]\n",
    "    filtered_ground_truth_labels = [reverse_label_encoding[gt] for gt in filtered_ground_truth]\n",
    "    print(classification_report(\n",
    "        filtered_ground_truth_labels,\n",
    "        filtered_predictions_labels,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "# Initialize the Word2Vec model\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")\n",
    "train_dataset = dataset['train']\n",
    "#validation_dataset = dataset['validation']\n",
    "#test_dataset = dataset['test']\n",
    "\n",
    "# Define label encoding\n",
    "label_encoding = {\"B-O\": 0, \"B-AC\": 1, \"B-LF\": 2, \"I-LF\": 3}\n",
    "\n",
    "# Prepare the data\n",
    "train_data = prepare_data(train_dataset, word2vec_model, label_encoding)\n",
    "#validation_data = prepare_data(validation_dataset, word2vec_model, label_encoding)\n",
    "#test_data = prepare_data(test_dataset, word2vec_model, label_encoding)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = data.DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "#validation_loader = data.DataLoader(validation_data, batch_size=16, shuffle=False)\n",
    "#test_loader = data.DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define model parameters\n",
    "input_size = word2vec_model.vector_size\n",
    "hidden_dim = 128\n",
    "output_dim = len(label_encoding)\n",
    "\n",
    "# Create and train the model\n",
    "model = RNNModel(input_size, hidden_dim, output_dim)\n",
    "train_model(model, train_loader, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "#evaluate_model(model, test_loader, label_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90a3be23-bf02-4094-8c89-79e79e2755d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, word2vec_model, label_encoding, file_path):\n",
    "    model_data = {\n",
    "        'input_size': model.rnn.input_size,\n",
    "        'hidden_dim': model.rnn.hidden_size,\n",
    "        'output_dim': model.fc.out_features,\n",
    "        'n_layers': model.rnn.num_layers,\n",
    "        'dropout': model.rnn.dropout,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'word2vec_model': word2vec_model,\n",
    "        'label_encoding': label_encoding\n",
    "    }\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(model_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5707ed3-7821-4cf3-9089-5e60f38189a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, word2vec_model, label_encoding, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d383422-d4e2-4c33-88c9-2ea8a897c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_model(file_path):\n",
    " #   with open(file_path, 'rb') as f:\n",
    "  #      model_data = pickle.load(f)\n",
    "#\n",
    " #   # Create the RNNModel instance\n",
    "  #  model = RNNModel(model_data['input_size'], model_data['hidden_dim'], model_data['output_dim'])\n",
    "#\n",
    " #   # Load the state_dict\n",
    "  #  model.load_state_dict(model_data['state_dict'])\n",
    "#\n",
    " #   # Retrieve word2vec_model and label_encoding\n",
    "  #  word2vec_model = model_data['word2vec_model']\n",
    "   # label_encoding = model_data['label_encoding']\n",
    "#\n",
    " #   return model, word2vec_model, label_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3f9ec6b-39a2-4fbd-9c25-ed437647883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-AC', 'B-O', 'B-LF', 'B-O', 'B-AC', 'B-O', 'B-LF', 'I-LF', 'B-O']\n"
     ]
    }
   ],
   "source": [
    "#model, word2vec_model, label_encoding = load_model('model.pkl')\n",
    "#example_tokens = [ \"KO\", \",\", \"knockout\", \";\", \"PSD\", \",\", \"postsynaptic\", \"density\", \".\" ]\n",
    "#predicted_labels = predict(example_tokens, model, word2vec_model, label_encoding)\n",
    "#print(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
